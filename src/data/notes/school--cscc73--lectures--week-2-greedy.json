{
  "id": "school--cscc73--lectures--week-2-greedy",
  "path": "school/cscc73/lectures/week-2-greedy",
  "title": "Week 2 - Greedy",
  "content": "# Lecture Notes\n\n**Fundamental Principle**: Don't think more than one step ahead.\n### Solving Gradient Descent\n\n**Continuous optimization**: find a point $x \\in R^n$ maximizing/minimizing some real function $f$\n- Infeasible except for special cases, only works on special cases\n\nTo minimize, move locally down. \n#### Problem\n\nGiven $x_0, L, \\epsilon \\in \\mathbb{R}$ where $\\epsilon > 0$ and a real differentiable function $f$ whose derivative at an arbitrary point can be evaluated at unit cost.\n\nWe want to output $x^* \\in I$ such that $| f(x^*) - f^* | \\leq \\epsilon$ where $f^* = min_{x ∈ I} {( f(x) )}$ and $I = [ x_0 - L , x_0 + L ]$\n\n**Rough Algorithm**\n\nLet $\\delta = \\delta(\\epsilon), t = t( L, \\epsilon)$\nFor $i = 0, \\ldots, t-1$\n\t$x_{i + 1} = x_i - f^`(x) \\dot \\delta$\nOutput $x_t$\n\nThe overall algorithm runs in $O(t(L,ε))$ steps + time it takes to compute $t$ and $\\delta$\n\n**This doesn't work all the time.** \n\nFor every $t, δ$, there exist $f$ and $(x_0, L, \\epsilon)$, such that the output $y$ of $GD_{t, \\delta}$\non input $(x_0, L, \\epsilon)$ with access to $f’$ satisfies $| f(y) - f^* | > ε$ \n- Just saying for all inputs, there is some case where we violate the problem constraint\n\nWe can cook up quite a lot of inputs that make this fail.\n### Convex Optimization\n\n**Theorem:** Fix $r \\in R$. Let $\\delta = 0.5, t(L, \\epsilon) = \\gamma(r/\\epsilon) \\cdot L^21$ \n\nLet $f: R \\rightarrow R$  be r-smoothly convex. Then the algorithm $GD_{t, \\delta}$ solves the optimization problem for $f$. \n##### TODO: Finish proof from photo\n\n### Interval Scheuduling\n\n**Setup**\n\nThink of a server scheduling requests that start at some time and take some time to process.\n\n- For an interval $I \\subseteq R$, denote $s(I) = min_{x \\in I}(x)$ and $f(I) = max_{x \\in I}(x)$\n- A set of intervals $I_1, \\ldots, I_n$ is valid if for each $i,j \\in [n]$ we have $I_i \\cap I_j = \\emptyset$ \n\t- Can only entertain one request at a time.\n\n**Goal**\n\n- Input: A set of intervals $S  = I_1, \\ldots, I_n \\subseteq [0, \\infty]$\n- Output: A valid $T \\subseteq S$ such that $|T| = t^*$ where $t^* = max_{\\textit{valid } T' \\subseteq S} (|T'|)$\n\t- Just find the largest possible valid subset of intervals\n\t- Continuing with the server analogy, maximize the number of requests our server can handle. \n#### Slower Solution\n\nRealize that simple greedy approaches don't work:\n- take shortest\n- take task that starts at the earliest time\n\n*Take the task that finishes the earliest*\n\nThe algorithm consists of the following steps:\n\n1. Sort the intervals $S$ by the right endpoints, in ascending order\n2. Let $T = \\emptyset$\n3. For each interval $I \\in S$ in ascending order\n\t1. If $T \\cup {I}$ is valid, let $T = T \\cup {I}$ \n4. Output $T$\n\nIt looks like it but this actually isn't linear TC (excluding the sort). This is because of the time it takes to check whether a set is valid which makes TC `O(n^2)`\n\nThe optimized version of this algorithm is `O(n log n)` with TC dominated by the sorting and non-sorting work only taking `O(n)`\n\nThis optimized version of the algorithm is:\n\n1. Sort the intervals $S$ by the right endpoints, in ascending order\n2. Let $T = \\emptyset$, $J = [-2, -1]$\n\t1. As we see in 2. \n3. For each interval $I \\in S$ in ascending order\n\t1. If $s(I) \\leq f(J)$: let $T \\cup {I}$ and $J = I$\n4. Output $T$\n##### Correctness\n\nLet's start by proving the `O(n^2)` solution.\n\nFor ease, assume that we index the intervals $I_1, \\ldots I_n$ are in the sorted order.\n\n- Def: A set $O$ of intervals is optimal if $O$ is valid and $|O| = t^*$\n- Def: For every $i \\in {0,\\ldots, n}$, let $T^I$ be the set of intervals the algorithm has obtained at each iteration $i$\n- Def: We say that $T^i$ can be forward extended to a set $O$ if there is $S \\subseteq { I_{i+1}, ..., I_n }$ such that $O = T^i \\cup S$\n###### Lemma\n\nFor every $i = 0, \\ldots, n$, there is an optimal set $O^i$ such that $T^i$ can be forward extended to $O^i$ \n\nProve with induction!\n\n**Base Case:**  $T^0 = \\emptyset$ so the claim trivially holds since all sets, including the optimal sets, are supersets of the empty set.\n\n**Induction Hypothesis**: Assume that $T^i$ can be forward extended to an optimal $O^i$\n\n**Induction Step**:\n\nWTS: $T^{i + 1}$ can be extended to an optimal $O^{i + 1}$\n\n**Case 1:** $T^{i + 1} = T^i$, that is to say that the interval $I_{i + 1}$ collides with some interval in $T^i$\n\nChoose $O^{i + 1} = O^i$   then $T^{i + 1} = T^i \\subseteq O^i = O^{i + 1}$ where  $T^i \\subseteq O^i$ is an application of the IH. We want to show that $I_{i + 1} \\notin O^{i + 1}$\n\nSince the algorithm didn't add $I_{i + 1}$, it must intersect with some interval in $T^i$.   $T^i \\subseteq O^i$ by IH. We chose $O^{i + 1} = O^i$  and since $O^{i + 1}$ is valid it must be the case that $I_{i + 1} \\notin O^{i + 1}$\n\n**Case 2**: $I^{i + 1} \\in T^i \\cap O^i$ \n\nLet $O^{i + 1} = O^i$. Since $O^{i + 1}$ only contains intervals from $T^i \\cup {I_{i+1}, \\ldots, I_n}$. Hence $T^{i + 1}$ can be extended to an optimal set $O^{i + 1}$\n\n**Case 3**: $I_{i + 1} \\in T^{i + 1} \\backslash O^i$\n\nLet $J \\in O^i$ such that $f(J) > f(I_{i + 1})$ and $J$ has minimal endpoints among intervals in $O^i$. Let $O^{i + 1} = O^i \\cup {I_{i + 1} \\backslash {J}}$. Note that $O^{i + 1}$ is optimal since $|O^{i + 1}| = |O^i|$ \n\n*Claim*: $T^{i + 1}$ can be extended to $O^{i + 1}$\n\n$T^{i + 1} = T \\cup {I_{i + 1}} \\subseteq O^i \\cup {I_{i + 1}} = O^{i + 1} \\cup {J}$\n\nSince the algorithm considered $I_{i + 1}$ and not $J$ in iteration $i + 1$, $J \\notin T^{i + 1}$ hence $T^{i + 1} \\subseteq O^{i + 1}$ \n\nSine $T$ can be extended to $O$\n\n$O^{i + 1} = O^i \\cup I_{i + 1} \\backslash J \\subseteq T \\cup \\{ I_{i + 1}, \\ldots I_n \\} \\backslash \\{J \\}  \\subseteq T^{i+ 1} \\cup \\{ I_{i + 1}, \\ldots I_n \\}$\n\nThus our claim holds.\n\n*Claim*: $O^{i + 1}$ is valid\n\nIt suffices to prove that $I_{i + 1} \\cap J' = \\emptyset$ where $J'$ is defined as $\\forall J' \\in O^i \\backslash \\{ J \\}$ \n\nFixing such a $J'$, if $f(J') < f(I_{i + 1})$ then our algorithm considered $J$ in an iteration proceeding $I_{i + 1}$ \n\nSince $J' \\in O^i$ and $O^i$ is valid then it means that $J$ is disjoint from all items in $T$, the algorithm added $J'$ to $T^j$ since we added $I_{i + 1}$ in iteration $i + 1$ then it must be the case that $I_{i + 1} \\cap J' = \\emptyset$ \n\n#### Optimized Interval Scheuduling\n\n1. Sort the intervals $S$ by right endpoints, in increasing order\n2. $T = \\emptyset$, $I_{last} = [ -2, -1]$\n3. For each interval $I \\in S$ in sorted order,\n\t1. If $s(I) > f(I_{last})$: let $T = T \\cup \\{ I \\}$ and let $I_{last} = I$\n4. Output $T$\n\nThis new implementation runs in time $O(n log n)$\n##### Proof of TC\n\nWe represent $T$ as an array of integers (indices of intervals in $T$) appearing after the input.\n- Sorting the input can be done in time $O(n log n)$\n- Loop has $n$ iterations\n\t- In each iteration, we compare constantly many integers and update constantly. Thus each iteration takes $O(1)$ time\n##### Proof of Lemma\n\nAt iteration $i \\in [n]$ of the algorithm, $T^{i - 1} \\cup \\{ I_i \\}$ is valid iff. $s(I_i) > f(I_i)$, where $I_i$ is the last interval added to $T^{i -1}$ prior to iteration $i$\n###### Proof of $\\rightarrow$ \n\nAssume $T^{i - 1} \\cup \\{ I_{i}\\}$ is valid. Since intervals are considered in ascending order of right endpoint, $f(I_i) \\geq f(I_{last})$. Hence $s(I_i) > f(I_{last})$ as otherwise they would intersect. \n###### Proof of $\\leftarrow$\n\nAssume $s(I_i) > f(I_i)$. Since the algorithm considers intervals in ascending order of right endpoints, and $I_{last}$ was the last interval added to $T$. $f(J) \\leq f(I_{last})$. $\\forall J \\in T^{i -1}, f(I_{last}) < s(I_i)$, hence $I_I \\cap J = \\emptyset, \\forall J \\in T^{i - 1}$  \n### Independent Set\n\nInput: graph $G = (V, E)$\nOutput: independent set $S \\subseteq V$ such that $|S| = s^*$ where $s^* = max_{\\textup{}}$\n# Kleinberg & Tardos - MST Algorithms\n\n## Problem\n\nGiven a set of vertices $V = \\{v_1, v_2, v_3, \\ldots v_n \\}$ and a set of *positive weighted* edges $E$ find and return the set of least expensive set of edges $T$ ($T \\subseteq E$) such that $G(V, T)$  is a connected graph. \n- It can be easily shown that $G(V, T)$ is a tree. \n\t- **Brief Proof**: A tree is a connected graph without cycles. Consider $G(V, T)$. If there exists an edge $e$ that causes a cycle, removing this edge will result in a cheaper set of edges that are still connected. Therefore $T$ is not minimal which is a contradiction.\n## Two Approaches\n\n### Kruskal's Algorithm\n\n1. Sort edges in ascending order based on cost\n2. Initialize $T$ \n3. Iterate over each edge $e$\n\t1. If adding $e$ to $T$ causes a cycle, skip it\n\t2. Else, $T = T \\cup e$\n\nActual Python Code:\n\n```python\n\n```\n\n**TC**: \n### Prim's Algorithm\n\n*This is kinda like Dijkstra's*\n\nThe minimum spanning tree is built gradually by adding edges one at a time. \n\n1. Choose a single vertex $v$ arbitrarily \n2. Choose the minimum weight edge outgoing from $v$ selected and add it to the spanning tree \n\t1. Now spanning tree has $v$ and some other vertex $u$\n3. Until we have $n -1$ edges, select and add the edge with minimal weight that connects one selected vertex with one unselected vertex. \n\nIn the end the constructed spanning tree will be minimal. \n\nIf the graph was originally not connected, then there doesn't exist a spanning tree, so the number of selected edges will be less than  $n - 1$ .\n\n**Actual Python Code:**\n\nIf the Graph is Sparse and implicit, has **TC** `O(E log V)`\n\n```python\nselected = [False] * n\nmin_heap = [(0, 0)]\nremaining_vertices = n\nmin_cost = 0\n\nwhile min_heap:\n\tcost, vertex = heapq.heappop(min_heap)\n\tif selected[vertex]:\n\t\tcontinue\n\tselected[vertex] = True\n\tremaining_vertices -= 1\n\tmin_cost += cost\n\t\n\tif remaining_vertices == 0:\n\t\treturn min_cost\n\t\n\tfor edge_cost, neighbour in graph[vertex]:\n\t\tif selected[neighbour]: continue\n\t\theapq.heappush(min_heap, (edge_cost, neighbour))\n```\n\nIf the Graph is dense/explicit (each vertex is connected to every other vertex). Then it makes more sense to compute costs on the fly:\n\n```python\nn = len(points)\nin_mst = [False] * n\nminDist = [float('inf')] * n\nminDist[0] = 0 # pick 0 arb.\nres = 0\n\t\t\nfor _ in range(n):\n\t# pick next node with smallest cost to connect\n\tnext_node = -1\n\tsmallest_cost = float('inf')\n\t\n\tfor node in range(n):\n\t\tif not in_mst[node] and minDist[node] < smallest_cost:\n\t\t\tsmallest_cost = minDist[node]\n\t\t\tnext_node = node\n\n\tin_mst[next_node] = True\n\tres += smallest_cost\n\n\tx1, y1 = points[next_node]\n\t# relax edges to all remaining nodes\n\tfor node in range(n):\n\t\tif not in_mst[node]:\n\t\t\tx2, y2 = points[node]\n\t\t\td = abs(x1 - x2) + abs(y1 - y2)\n\t\t\tif d < minDist[node]:\n\t\t\t\tminDist[node] = d\nreturn res\n```\n\n#### When to use Prim's over Kruskal's\n\nFrom this StackOverflow [thread](https://stackoverflow.com/questions/1195872/when-should-i-use-kruskal-as-opposed-to-prim-and-vice-versa):\n\n> Prim's algorithm is significantly faster in the limit when you've got a really dense graph with many more edges than vertices. Kruskal performs better in typical situations (sparse graphs) because it uses simpler data structures.\n\n\n\n\n\n",
  "metadata": {
    "date": "2026-02-05",
    "updated": "2026-02-05"
  },
  "images": [],
  "links": []
}