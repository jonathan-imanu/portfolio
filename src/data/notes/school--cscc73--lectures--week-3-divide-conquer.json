{
  "id": "school--cscc73--lectures--week-3-divide-conquer",
  "path": "school/cscc73/lectures/week-3-divide-conquer",
  "title": "Week 3 - Divide & Conquer",
  "content": "Given a problem:\n\n1. Partition it into disjoint sub-problems (typically, of the same type)\n2. Solve each sub-problem (typically, by recursion)\n3. Combine the solutions\n\nThe creative part is how to divide and how to combine.\n\nTwo famous divide and conquer algorithms are quicksort and merge sort. \n\n## Integer Multiplication\n\nAssuming we can't multiply integers, add or shift  integers at unit cost \n\n**Input**: Two integers $x = \\sum_{i \\in \\{ 0, \\ldots, n - 1\\}} x_i \\cdot 2^i$ and $y = \\sum_{i \\in \\{ 0, \\ldots, n - 1\\}} y_i \\cdot 2^i$ \n**Output:** The multiplication $x \\cdot y$, where arithmetic is over the integers, represented by $2n-1$ bits\n\nThe basic grade school algorithm has TC $O(n^2)$\n### Karatsuba's DC Algorithm\n\nSimplifying assumption: $n$ is a power of two\n\n**LSB comes first in this representation for whatever reason**.\n\nThe product $x \\cdot y$ can be written as follows:\n$$\\begin{align*}\nx \\cdot y\n&= \\left(x_l + 2^{n/2} x_r\\right)\n   \\left(y_l + 2^{n/2} y_r\\right) \\\\[6pt]\n&= x_l y_l\n   + x_l y_r \\cdot 2^{n/2}\n   + y_l x_r \\cdot 2^{n/2}\n   + x_r y_r \\cdot 2^{n} \\\\[6pt]\n&= x_l y_l\n   + \\left(x_l y_r + y_l x_r\\right) \\cdot 2^{n/2}\n   + x_r y_r \\cdot 2^{n}\n\\end{align*}$$\n#### First Attempt\n\n- We can recursively compute Mult$(x_l, y_l)$, Mult$(x_l, y_r)$, Mult$(x_r, y_l)$, Mult$(x_r, y_r)$\n- And then compute $x_l y_l \\left(x_l y_r + y_l x_r\\right) \\cdot 2^{n/2} x_r y_r \\cdot 2^{n}$ in $O(n)$ time\n\t- Remember that adding and shifting both take $O(n)$ time for an input with $n$ bits\n- Our recursive tree will have depth $log_2 n$\n\nThe TC is $O(cn^2)$\n\nWe will quickly prove this:\n##### Proof that the TC is indeed bad\n\nThere is $c > 1$ such that on length $n$ that is a power of two the algorithm runs in time at least $cn^2$.\n\nLet $c = $\n- Remark: While doing the proof, we initially left this blank and decided we'll get a good value later.\nWe prove by induction.\n\n**Base Case**: $n = 1$\n\nThe algorithm runs in some constant time $c_1$. \n- Remark: At this point, we know we must choose a $c$ that is at least $c_1$\n\n**Induction Step**\n\nAssume the algorithm takes time $\\geq cn^2$ on $n$-bit inputs. \nWe prove it takes time $\\geq$ $c(2n)^2$ on $2n$-bit inputs.\nSince the algorithm calls itself 4 times on inputs of length $n$ & by the IH, its runtime is $4 \\cdot cn^2 = c(2n)^2$\n#### The Key Trick\n\n$$\\begin{align*}\nx \\cdot y\n&= \\left(x_l + 2^{n/2} x_r\\right)\n   \\left(y_l + 2^{n/2} y_r\\right) \\\\[6pt]\n&= x_l y_l\n   + x_l y_r \\cdot 2^{n/2}\n   + y_l x_r \\cdot 2^{n/2}\n   + x_r y_r \\cdot 2^{n} \\\\[6pt]\n&= x_l y_l\n   + \\left(x_l y_r + y_l x_r\\right) \\cdot 2^{n/2}\n   + x_r y_r \\cdot 2^{n} \\\\[6pt]\n&= x_l y_l\n   + \\left((x_l + x_r) \\cdot (y_r + y_l) -x_l \\cdot y_l - x_r \\cdot y_r \\right) \\cdot 2^{n/2}\n   + x_r y_r \\cdot 2^{n}\n\\end{align*}$$\nThe final step makes it such that we only have to do three multiplications!\n- The key optimization here becomes clear when we look that the tree made by the recursive calls. We now have three recursive calls instead of 4!\n\n##### Proving TC \n\nWe want to prove that time algorithm runs in TC $O(n^{1 + log(3/2)})$ \n\nLet $c = c_1 + c_2$\n- Remark: We left this blank\nLet $c'= c_2$\n- Remark: We left this blank\n\nWe prove by induction that $\\forall n$ that are powers of two, the algorithm runs in time $O(n^{1 + log(3/2)}) - c'n$  \n\n**Base Case**: $n = 1$\n\nThe algorithm runs in some constant time $c_1$\n- Basic algebra will some that this holds\n\n**Induction Step**\n\nAssume the algorithm takes time $\\leq c\\cdot (2n)^{1 + log(3/2)} - c'n$  on $n$ bit inputs. \n\nSince the algorithm calls itself 3 times on inputs of length $n$ ($+ c 2n$ time for combining) & by the IH, it's runtime is \n$$\\leq 3 \\cdot cn^{1 + log(3/2)} - 3c'n + c_2n$$\n$$= \\frac{3}{2^{1 + log(3/2)}} c(2n)^{1 + log(3/2)} - (3c' -c_2)n $$\n$$= c \\cdot (2n)^{1 + log(3/2)} - c'(2n)$$\n\n## Master Theorem(s)\n\n**Informally:**\n\nLet $T(n) = a \\cdot T(n /b)$ + (relatively small \"combine\" runtime) then $T(n) = O(n^c)$ where $c = log_b(a)$\n- We make $a$ recursive calls, at each recursive call we divide the input by $b$\n\t- So at the $i$th level, we make $a^i$ recursive calls with an input size of $n / b^i$ \n\t- We need to hit $\\log_b{n}$ levels before reaching a constant size problem\n\t- We only care about the last level since it dominates\n\nHere's some work showing that the sum of costs from the last level of the recursive tree will be equal to \n\n$(\\frac{a}{b})^{log_b(n)} \\cdot c \\cdot n$\n$= b^{log_b(\\frac{a}{b}) \\log_b(n)}$ \n$= c \\cdot n^{\\log_b(\\frac{a}{b})}$\n### Formally\n\nFor $a,b,d,e \\leq 1$, let $T: N \\rightarrow N$ such that $T(n) \\leq a \\cdot T(\\lceil n / b \\rceil) + e \\cdot n^d$ for any sufficiently large $n$. Let $c = \\log_b(a)$.\n\n1. If $c$ > $d$, then $T(n) = O(n^c)$\n2. If $c = d$, then $T(n) = O(n^c \\cdot \\log(n))$ \n\t1. This is the annoyingly tight case\n3. If $c < d$, then $T(n) = O(n^d)$ \n\n##### Using the Master Theorem with Merge Sort\n\nWe make two recursive calls to input length $n / 2$. So $a = 2$ and $b = 2$. Thus $c = \\log_2(2) = 1$.\nThe combine step is in linear time, therefore $d = 1$. \n\nBy the master theorem, this has running time $O(n \\log n)$.\n## Matrix Multiplication\n\n#### Approach 1: Brute Force\n\n**Input**: Two matrices $A$ and $B$ of size $n \\times n$, each with $n^2$ elements. \n**Output**: $C = AB$, an $n \\times n$ matrix\n\nWe know that an element of $C$, call it $c_{ij} = \\sum a_{ik} \\cdot b_{kj}$ \n- This takes time $O(n)$ for $n$ multiplications and additions\n- We have $n^2$ elements to figure out \n- Thus the total brute force TC will be $O(n^3)$\n#### Approach 2: A Bad Divide & Conquer\n\nWe assume that $n = 2^x$ for some integer $x$\n\nThe idea is we divide the matrix into four quadrants.\n\nThen we can join them back up with\n\n$C = (A_{11}B_{11} + A_{12}B_{21})$\n\n## Polynomials\n\nWe care about univariate complex polynomials. These are defined like: $p(x) = \\sum_{i = 0}^{d} c_i \\cdot x^i$ \n#### Fundamental Theorem of Algebra\n\nA polynomial of degree $d$ either is constantly zero or has at most $d$ roots.\n##### Cor. \n\nFor every distinct $x_1 \\ldots x_{d+1} \\in \\mathbb{C}$ and every $y_1 \\ldots y_{d + 1} \\in \\mathbb{C}$ there <u>exists</u> a <u>unique</u> polynomial of degree at most $d$ such that $p(x_i) = y_i \\forall i$\n\n**Proof**\n\nThere exists a $p(x) = \\sum_{i \\in [d + 1]} y_i \\cdot \\prod_{j \\in [d + 1] \\ i} \\frac{x - x_i}{x_i - x_j}$ \n- The part of $\\prod$ is a function that gives 1 for $x_i$ and 0 for $x_j$ when $j \\neq i$ \n\nLet $p_1$ and $p_2$ be two degree $d$ such that $p_1(x_i) = p_2(x_i) = y_i \\forall i$ \n\nDefine $p(x) = p_1 + p_2$ THEN:\n1. $deg(p) \\leq d$ \n2. The number of roots of $p$ is at least $d + 1$\n\nBut this violates the FTA! So $p(x)$ must be constantly zero. So there can only be one polynomial of at most $d$ such that $p(x_i) = y_i \\forall i$.\n#### Eval. Representation\n\nFix distinct $x_0 \\ldots x_{n - 1}$. Then let the $(x_0, y_0)$ pair mean that $y_0$ is the value of the polynomial at $x_0$. \n- We know how to represent things in coefficient representation.\n##### Converting to Coefficient Representation - Using Lagrange (Naive)\n\n$P(x) = \\sum_{i=1}^n y_i \\ell_i(x), \\quad \\ell_i(x) = \\prod_{j \\neq i} \\frac{x - x_j}{x_i - x_j}$\n\nHas TC $O(n^2)$\n##### Converting from Coefficient Representation to Eval. \n\nThis is more straightforward, for each $x_i \\in \\{x_1, \\ldots, x_n \\}$ just plug it in.\n- $n$ points and we'll have to make $n$ iterations (one for each coefficient) at every iteration, hence we have TC $O(n^2)$ \n### The Evaluation Problem\n\n**Input**: A univariate polynomial $P: \\mathbb{C} \\rightarrow \\mathbb{C}$ of degree at most $n - 1$ represented as $n$ coefficients, and $n$ distinct points $x_1, \\ldots x_n \\in \\mathbb{C}$\n\n**Output**: The evaluations of $P$ at $x_1, \\ldots, x_n$, $(x_1, P(x_1)),\\ldots, (x_n,(P(x_n)))$\n### The Interpolation Problem\n\n**Input**: A univariate polynomial $P: \\mathbb{C} \\rightarrow \\mathbb{C}$ of degree at most $n - 1$ represented as evaluations at $n$ distinct points in $\\mathbb{C}$, $(x_1, P(x_1)),\\ldots, (x_n,(P(x_n)))$\n\n**Output**: The $n$ coefficients of $P$\n\n- With coefficient representation, it is easy to see that an algorithm that solves this problem would have TC `O(n)`\n- With the evaluation representation, it is fastest to convert to coefficient \n# DFT: Discrete Fourier Transform (DFT)\n\n${DFT}_{n}$ \n\n- **Input**: The coefficients of $P$ of degree at most $n - 1$\n- **Output**: $P(\\omega_{n}^{0}, \\ldots, \\omega_{n}^{n})$ \n\n${DFT}_{n}^{-1}$ \n\n- **Input**: $P(\\omega_{n}^{0}), \\ldots P(\\omega_{n}^{n})$\n- **Output**: The coefficients of $P$ of degree at most $n - 1$\n### What are the Omega's?\n\n$\\boxed{\\omega_n = e^{2\\pi i / n}}$ \n\n- $\\omega_n$ is a **primitive** $n^{th}$ root of unity\n- This means:\n    - $\\omega_n^n = 1$\n    - $\\omega_n^k \\neq 1$ for 0 < k < n\n        \nAll powers $\\omega_n^0, \\omega_n^1, \\dots, \\omega_n^{n-1}$ are the **distinct complex roots of unity**.\n\nWe use these because $|(\\omega_n)^2| = |\\omega_{n}|/2$, at the end we'll end up with 1. \n## Naive Approaches for a D&C Algorithm\n\n#### Approach 1: Split the List of Points\n\n**Idea:** Split the list of points:\n\n$W_{left} = (\\omega_{n}^{0}, \\ldots, \\omega_{n}^{n/2 - 1}))$ \n$W_{right} = (\\omega_{n}^{n/2}, \\ldots, \\omega_{n}^{n}))$\n#### Approach 2: Split the Polynomial\n\n**Idea**: Split the polynomial:\n\n$P(x) = P_{left}(x) + P_{right}(x^{n/2})$\n### Analysis\n\nThe same problem exists for both of these algorithms: the D&C step doesn't do anything! The combine step is still expensive and the algorithm still takes time $O(n^2)$\n## Cooley-Turkey FFT Algorithm (coef. to eval.)\n\n\n## Using FFT to compute Inverse DFT (eval. to coef.)\n\n### Intuition\n\nUse the Vandermonde matrix. \n### Basic Steps\n\n1. Let $f(x) = \\sum_{i = 0}^{n - 1} P(\\omega_{n}^{i} x^i)$\n2. Compute DFT(f) using FFT\n\t1. Time $O(n \\log n)$\n3. Output $c_0, \\ldots, c_{n-1}$ such that $c_i = \\frac{1}{n}f(\\omega_{n}^{n - i})$\n\n\n\n\n",
  "metadata": {
    "date": "2026-02-05",
    "updated": "2026-02-05"
  },
  "images": [],
  "links": []
}