{
  "id": "school--cscc73--lectures--week-3-divide-conquer",
  "path": "school/cscc73/lectures/week-3-divide-conquer",
  "title": "Week 3 - Divide & Conquer",
  "content": "Given a problem:\n\n1. Partition it into disjoint sub-problems (typically, of the same type)\n2. Solve each sub-problem (typically, by recursion)\n3. Combine the solutions\n\nThe creative part is how to divide and how to combine.\n\nTwo famous divide and conquer algorithms are quicksort and merge sort. \n\n## Integer Multiplication\n\nAssuming we can't multiply integers, add or shift  integers at unit cost \n\n**Input**: Two integers $x = \\sum_{i \\in \\{ 0, \\ldots, n - 1\\}} x_i \\cdot 2^i$ and $y = \\sum_{i \\in \\{ 0, \\ldots, n - 1\\}} y_i \\cdot 2^i$ \n**Output:** The multiplication $x \\cdot y$, where arithmetic is over the integers, represented by $2n-1$ bits\n\nThe basic grade school algorithm has TC $O(n^2)$\n### Karatsuba's DC Algorithm\n\nSimplifying assumption: $n$ is a power of two\n\n**LSB comes first in this representation for whatever reason**.\n\nThe product $x \\cdot y$ can be written as follows:\n$$\\begin{align*}\nx \\cdot y\n&= \\left(x_l + 2^{n/2} x_r\\right)\n   \\left(y_l + 2^{n/2} y_r\\right) \\\\[6pt]\n&= x_l y_l\n   + x_l y_r \\cdot 2^{n/2}\n   + y_l x_r \\cdot 2^{n/2}\n   + x_r y_r \\cdot 2^{n} \\\\[6pt]\n&= x_l y_l\n   + \\left(x_l y_r + y_l x_r\\right) \\cdot 2^{n/2}\n   + x_r y_r \\cdot 2^{n}\n\\end{align*}$$\n#### First Attempt\n\n- We can recursively compute Mult$(x_l, y_l)$, Mult$(x_l, y_r)$, Mult$(x_r, y_l)$, Mult$(x_r, y_r)$\n- And then compute $x_l y_l \\left(x_l y_r + y_l x_r\\right) \\cdot 2^{n/2} x_r y_r \\cdot 2^{n}$ in $O(n)$ time\n\t- Remember that adding and shifting both take $O(n)$ time for an input with $n$ bits\n- Our recursive tree will have depth $log_2 n$\n\nThe TC is $O(cn^2)$\n\nWe will quickly prove this:\n##### Proof that the TC is indeed bad\n\nThere is $c > 1$ such that on length $n$ that is a power of two the algorithm runs in time at least $cn^2$.\n\nLet $c = $\n- Remark: While doing the proof, we initially left this blank and decided we'll get a good value later.\nWe prove by induction.\n\n**Base Case**: $n = 1$\n\nThe algorithm runs in some constant time $c_1$. \n- Remark: At this point, we know we must choose a $c$ that is at least $c_1$\n\n**Induction Step**\n\nAssume the algorithm takes time $\\geq cn^2$ on $n$-bit inputs. \nWe prove it takes time $\\geq$ $c(2n)^2$ on $2n$-bit inputs.\nSince the algorithm calls itself 4 times on inputs of length $n$ & by the IH, its runtime is $4 \\cdot cn^2 = c(2n)^2$\n#### The Key Trick\n\n$$\\begin{align*}\nx \\cdot y\n&= \\left(x_l + 2^{n/2} x_r\\right)\n   \\left(y_l + 2^{n/2} y_r\\right) \\\\[6pt]\n&= x_l y_l\n   + x_l y_r \\cdot 2^{n/2}\n   + y_l x_r \\cdot 2^{n/2}\n   + x_r y_r \\cdot 2^{n} \\\\[6pt]\n&= x_l y_l\n   + \\left(x_l y_r + y_l x_r\\right) \\cdot 2^{n/2}\n   + x_r y_r \\cdot 2^{n} \\\\[6pt]\n&= x_l y_l\n   + \\left((x_l + x_r) \\cdot (y_r + y_l) -x_l \\cdot y_l - x_r \\cdot y_r \\right) \\cdot 2^{n/2}\n   + x_r y_r \\cdot 2^{n}\n\\end{align*}$$\nThe final step makes it such that we only have to do three multiplications!\n- The key optimization here becomes clear when we look that the tree made by the recursive calls. We now have three recursive calls instead of 4!\n\n##### Proving TC \n\nWe want to prove that time algorithm runs in TC $O(n^{1 + log(3/2)})$ \n\nLet $c = c_1 + c_2$\n- Remark: We left this blank\nLet $c'= c_2$\n- Remark: We left this blank\n\nWe prove by induction that $\\forall n$ that are powers of two, the algorithm runs in time $O(n^{1 + log(3/2)}) - c'n$  \n\n**Base Case**: $n = 1$\n\nThe algorithm runs in some constant time $c_1$\n- Basic algebra will some that this holds\n\n**Induction Step**\n\nAssume the algorithm takes time $\\leq c\\cdot (2n)^{1 + log(3/2)} - c'n$  on $n$ bit inputs. \n\nSince the algorithm calls itself 3 times on inputs of length $n$ ($+ c 2n$ time for combining) & by the IH, it's runtime is \n$$\\leq 3 \\cdot cn^{1 + log(3/2)} - 3c'n + c_2n$$\n$$= \\frac{3}{2^{1 + log(3/2)}} c(2n)^{1 + log(3/2)} - (3c' -c_2)n $$\n$$= c \\cdot (2n)^{1 + log(3/2)} - c'(2n)$$\n\n## Master Theorem(s)\n\n**Informally:**\n\nLet $T(n) = a \\cdot T(n /b)$ + (relatively small \"combine\" runtime) then $T(n) = O(n^c)$ where $c = log_b(a)$\n\n\n\n",
  "metadata": {
    "date": "2026-01-27",
    "updated": "2026-01-27"
  },
  "images": [],
  "links": []
}